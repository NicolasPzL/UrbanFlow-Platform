# Tutorial de Instalaci√≥n del Chatbot UrbanFlow

Este tutorial te guiar√° paso a paso para instalar y ejecutar el chatbot de UrbanFlow en tu entorno local.

## üìã Tabla de Contenidos

1. [Requisitos Previos](#requisitos-previos)
2. [Instalaci√≥n de Ollama](#instalaci√≥n-de-ollama)
3. [Configuraci√≥n de la Base de Datos](#configuraci√≥n-de-la-base-de-datos)
4. [Configuraci√≥n del Entorno](#configuraci√≥n-del-entorno)
5. [Instalaci√≥n de Dependencias](#instalaci√≥n-de-dependencias)
6. [Ejecuci√≥n de los Servicios](#ejecuci√≥n-de-los-servicios)
7. [Probar el Chatbot](#probar-el-chatbot)
8. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## üîß Requisitos Previos

Antes de comenzar, aseg√∫rate de tener instalado:

- **Node.js** (versi√≥n >= 18.18.0) - [Descargar Node.js](https://nodejs.org/)
- **Python** (versi√≥n 3.8 o superior) - [Descargar Python](https://www.python.org/downloads/)
- **PostgreSQL** (versi√≥n 12 o superior) - [Descargar PostgreSQL](https://www.postgresql.org/download/)
- **Git** - [Descargar Git](https://git-scm.com/downloads)

### Verificar Instalaciones

```bash
# Verificar Node.js
node --version

# Verificar Python
python --version

# Verificar PostgreSQL
psql --version

# Verificar Git
git --version
```

---

## ü§ñ Instalaci√≥n de Ollama

El chatbot utiliza Ollama con el modelo Llama 3 para procesar las consultas en lenguaje natural.

### Paso 1: Descargar Ollama

1. Ve a la p√°gina oficial de Ollama: **https://ollama.ai/download**
2. Descarga el instalador para tu sistema operativo:
   - **Windows**: Ejecuta el instalador `.exe`
   - **macOS**: Ejecuta el instalador `.dmg`
   - **Linux**: Sigue las instrucciones en la p√°gina

### Paso 2: Instalar Ollama

**Windows:**
- Ejecuta el archivo `OllamaSetup.exe` descargado
- Sigue el asistente de instalaci√≥n
- Ollama se instalar√° y ejecutar√° autom√°ticamente

**macOS:**
- Abre el archivo `.dmg` descargado
- Arrastra Ollama a la carpeta Aplicaciones
- Ejecuta Ollama desde Aplicaciones

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Paso 3: Verificar Instalaci√≥n

Abre una terminal o PowerShell y ejecuta:

```bash
ollama --version
```

Deber√≠as ver la versi√≥n de Ollama instalada.

### Paso 4: Descargar el Modelo Llama 3

Ejecuta el siguiente comando para descargar el modelo:

```bash
ollama pull llama3
```

Este proceso puede tardar varios minutos dependiendo de tu conexi√≥n a internet. El modelo tiene aproximadamente 4.7 GB.

### Paso 5: Verificar que Ollama est√° Corriendo

Abre tu navegador y ve a: **http://localhost:11434**

Si ves informaci√≥n sobre Ollama, est√° funcionando correctamente.

Tambi√©n puedes verificar con:

```bash
ollama list
```

Deber√≠as ver `llama3` en la lista de modelos descargados.

---

## üóÑÔ∏è Configuraci√≥n de la Base de Datos

### Paso 1: Crear la Base de Datos

Abre PostgreSQL (pgAdmin, psql, o tu cliente preferido) y crea la base de datos:

```sql
CREATE DATABASE Urbanflow_db;
```

### Paso 2: Verificar Conexi√≥n

Aseg√∫rate de que PostgreSQL est√© corriendo y puedas conectarte con tus credenciales.

---

## ‚öôÔ∏è Configuraci√≥n del Entorno

### Paso 1: Crear el Archivo .env

En la ra√≠z del proyecto, crea un archivo llamado `.env` (si no existe) con el siguiente contenido:

```env
# Configuraci√≥n de Base de Datos
DB_USER=postgres
DB_PASSWORD=tu_contrase√±a_postgres
DB_HOST=localhost
DB_PORT=5432
DB_NAME=Urbanflow_db

# Configuraci√≥n del Chatbot (Ollama)
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3

# Configuraci√≥n del Servicio Analytics
DEBUG=false
LOG_LEVEL=INFO

# Configuraci√≥n del Chatbot
CHATBOT_MAX_CONTEXT_MESSAGES=10
CHATBOT_SQL_ROW_LIMIT=100
CHATBOT_ENABLE_ML_ANALYSIS=true
```

**‚ö†Ô∏è Importante:** 
- Reemplaza `tu_contrase√±a_postgres` con tu contrase√±a real de PostgreSQL
- Si tu base de datos tiene otro nombre o usuario, actualiza esos valores

---

## üì¶ Instalaci√≥n de Dependencias

### Paso 1: Instalar Dependencias de Node.js

Desde la ra√≠z del proyecto:

```bash
npm install
```

### Paso 2: Crear y Activar Entorno Virtual de Python

**Windows (PowerShell):**
```powershell
cd microservices/analytics
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Windows (CMD):**
```cmd
cd microservices\analytics
python -m venv venv
venv\Scripts\activate.bat
```

**macOS/Linux:**
```bash
cd microservices/analytics
python3 -m venv venv
source venv/bin/activate
```

### Paso 3: Instalar Dependencias de Python

Con el entorno virtual activado:

```bash
pip install -r requirements.txt
```

Este proceso puede tardar varios minutos.

### Paso 4: Instalar Dependencias del Frontend

Desde la ra√≠z del proyecto:

```bash
npm --prefix "views" run build
```

---

## üöÄ Ejecuci√≥n de los Servicios

###  Ejecutar Todo 

#### Terminal 1(raiz del proyecto):
```bash
npm run dev
```

#### Terminal 2:
**Windows:**
```powershell
cd microservices/analytics
.\venv\Scripts\Activate.ps1
python -m uvicorn app.main:app --reload --port 8001
```

**macOS/Linux:**
```bash
cd microservices/analytics
source venv/bin/activate
python -m uvicorn app.main:app --reload --port 8001
```

### Paso 1: Aseg√∫rate de que Ollama est√© Corriendo

Antes de ejecutar los servicios, aseg√∫rate de que Ollama est√© activo:

```bash
# Verificar que Ollama est√° corriendo
curl http://localhost:11434/api/version
```

O simplemente abre tu navegador en: http://localhost:11434

### Paso 2: Verificar que Todos los Servicios Est√©n Corriendo

Abre tu navegador y verifica los siguientes endpoints:

- **Backend Node.js**: http://localhost:3000/health
- **Servicio Analytics**: http://localhost:8001/health

---

## üí¨ Probar el Chatbot

### Paso 1: Acceder a la Aplicaci√≥n

1. Abre tu navegador en: **http://localhost:3000**

### Paso 2: Iniciar Sesi√≥n

1. Haz clic en **"Iniciar Sesi√≥n"**
2. Ingresa tus credenciales (debes tener un usuario creado en la base de datos)


### Paso 3: Abrir el Chatbot

1. En el **Navbar** (barra superior), ver√°s el bot√≥n **"Asistente IA"** al lado del bot√≥n "Cerrar Sesi√≥n"
2. Haz clic en **"Asistente IA"**
3. Se abrir√° una ventana del chatbot en la esquina inferior derecha

### Paso 4: Hacer Preguntas de Prueba

Puedes probar con estas preguntas:

- **Preguntas informativas:**
  - "¬øQu√© hace UrbanFlow?"
  - "¬øC√≥mo funciona el sistema?"

- **Consultas de datos:**
  - "¬øCu√°ntas cabinas est√°n operativas?"
  - "Mu√©strame las mediciones recientes del sensor 1"
  - "¬øCu√°l es el valor promedio de RMS hoy?"

- **An√°lisis:**
  - "¬øCu√°les sensores tienen los niveles de vibraci√≥n m√°s altos?"
  - "Genera un reporte de salud del sistema"

- **Reportes:**
  - "Dame un reporte completo del sistema"

---

## üîç Verificaci√≥n de Estado

### Verificar Estado de Ollama

```bash
# Ver modelos instalados
ollama list

# Verificar que Ollama est√° corriendo
curl http://localhost:11434/api/version

# Probar el modelo directamente
ollama run llama3 "Hola, ¬øc√≥mo est√°s?"
```

### Verificar Estado del Servicio Analytics

Abre: **http://localhost:8001/health**

Deber√≠as ver algo como:
```json
{
  "status": "healthy",
  "service": "UrbanFlow Analytics Service",
  "version": "1.0.0",
  "chatbot": {
    "initialized": true,
    "provider": "ollama",
    "model": "llama3"
  }
}
```

### Verificar Estado del Backend

Abre: **http://localhost:3000/health**

---

## üõ†Ô∏è Soluci√≥n de Problemas

### Problema: "Ollama no se reconoce como comando"

**Soluci√≥n:**
- Aseg√∫rate de haber instalado Ollama correctamente
- Reinicia tu terminal despu√©s de la instalaci√≥n
- En Windows, es posible que necesites agregar Ollama al PATH manualmente
- Verifica que Ollama est√© corriendo: abre el navegador en http://localhost:11434

### Problema: "Modelo llama3 no encontrado"

**Soluci√≥n:**
```bash
# Descargar el modelo
ollama pull llama3

# Verificar que se descarg√≥
ollama list
```

### Problema: "Error: No module named 'langchain'"

**Soluci√≥n:**
```bash
cd microservices/analytics
# Aseg√∫rate de tener el entorno virtual activado
.\venv\Scripts\Activate.ps1  # Windows
# o
source venv/bin/activate  # macOS/Linux

# Reinstalar dependencias
pip install -r requirements.txt
```

### Problema: "Error de conexi√≥n a la base de datos"

**Soluci√≥n:**
1. Verifica que PostgreSQL est√© corriendo
2. Verifica las credenciales en el archivo `.env`
3. Verifica que la base de datos `Urbanflow_db` exista:
   ```sql
   SELECT datname FROM pg_database WHERE datname = 'Urbanflow_db';
   ```

### Problema: "Error: OPENAI_API_KEY not found"

**Soluci√≥n:**
Este error no deber√≠a aparecer si est√°s usando Ollama. Aseg√∫rate de que en tu `.env` tengas:
```env
LLM_PROVIDER=ollama
```

### Problema: "El chatbot no responde o da errores"

**Soluci√≥n:**
1. Verifica que Ollama est√© corriendo: http://localhost:11434
2. Verifica los logs del servicio Analytics (terminal donde est√° corriendo uvicorn)
3. Verifica los logs del backend Node.js
4. Revisa la consola del navegador (F12) para ver errores del frontend

### Problema: "El bot√≥n 'Asistente IA' no aparece"

**Soluci√≥n:**
1. Aseg√∫rate de estar autenticado (haz clic en "Iniciar Sesi√≥n")
2. Verifica que el frontend se haya recargado despu√©s de los cambios
3. Revisa la consola del navegador para ver si hay errores de JavaScript

### Problema: "Error: UPSTREAM_ERROR"

**Soluci√≥n:**
1. Verifica que el servicio Analytics est√© corriendo en el puerto 8001
2. Verifica la configuraci√≥n en `app.js`: `ANALYTICS_BASE_URL=http://localhost:8001/api`
3. Verifica que no haya un firewall bloqueando la conexi√≥n

### Problema: "El chatbot dice que no tiene informaci√≥n"

**Soluci√≥n:**
1. Verifica que la base de datos tenga datos (tablas `mediciones`, `sensores`, `cabinas`, etc.)
2. Si la base de datos est√° vac√≠a, el chatbot puede no tener informaci√≥n para consultar
3. Verifica los logs del servicio Analytics para ver errores espec√≠ficos

---

## üìù Comandos √ötiles

### Detener Todos los Servicios

Presiona `Ctrl + C` en cada terminal donde est√©n corriendo los servicios.

### Ver Logs del Servicio Analytics

Los logs aparecen en la terminal donde ejecutaste uvicorn. Busca mensajes como:
- `"Initializing LLM with provider: ollama"`
- `"Connecting to Ollama at: http://localhost:11434"`
- `"Ollama client initialized successfully"`

### Reiniciar Ollama

Si necesitas reiniciar Ollama:

**Windows:**
- Cierra la aplicaci√≥n Ollama desde la bandeja del sistema
- Vuelve a abrirla desde el men√∫ de inicio

**macOS/Linux:**
```bash
# Detener
pkill ollama

# Iniciar (se inicia autom√°ticamente al usar ollama pull/run)
ollama serve
```

---

## üéØ Pr√≥ximos Pasos

Una vez que tengas todo funcionando:

1. **Explora las capacidades del chatbot:**
   - Haz preguntas sobre el sistema
   - Consulta datos espec√≠ficos de sensores y cabinas
   - Genera reportes de salud del sistema

2. **Personaliza el chatbot:**
   - Edita los prompts en `microservices/analytics/app/core/prompts.py`
   - Ajusta las configuraciones en el archivo `.env`

3. **Contribuye:**
   - Reporta bugs o sugiere mejoras
   - Mejora la documentaci√≥n
   - Agrega nuevas funcionalidades

---

## üìö Recursos Adicionales

- **Documentaci√≥n de Ollama**: https://ollama.ai/docs
- **Modelo Llama 3**: https://ollama.ai/library/llama3
- **LangChain Documentation**: https://python.langchain.com/
- **FastAPI Documentation**: https://fastapi.tiangolo.com/

---

## ‚ùì Preguntas Frecuentes

**P: ¬øNecesito una conexi√≥n a internet para usar el chatbot?**
R: Solo necesitas internet para descargar Ollama y el modelo Llama 3. Una vez descargado, todo funciona localmente.

**P: ¬øPuedo usar otro modelo de Ollama adem√°s de llama3?**
R: S√≠, solo necesitas cambiar `MODEL_NAME` en el archivo `.env` y descargar el modelo con `ollama pull <nombre-modelo>`.

**P: ¬øCu√°nto espacio en disco necesito?**
R: El modelo Llama 3 ocupa aproximadamente 4.7 GB. Adem√°s, necesitas espacio para Python, Node.js y PostgreSQL.

**P: ¬øEl chatbot funciona sin base de datos?**
R: No, el chatbot necesita acceso a la base de datos PostgreSQL para consultar informaci√≥n del sistema.

---

## üìû Soporte

Si encuentras problemas que no se resuelven con este tutorial:

1. Revisa los logs de todos los servicios
2. Verifica que todas las dependencias est√©n instaladas correctamente
3. Aseg√∫rate de que todos los servicios est√©n corriendo en los puertos correctos
4. Contacta al equipo de desarrollo para soporte adicional

---

**¬°Disfruta probando el chatbot de UrbanFlow! üöÄ**

